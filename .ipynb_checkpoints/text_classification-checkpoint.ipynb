{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bsoup\n",
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eachFile(filepath):\n",
    "    \"\"\"\n",
    "    get all the files in the dir\n",
    "    \"\"\"\n",
    "    pathDir =  os.listdir(filepath)\n",
    "    files = []\n",
    "    for allDir in pathDir:\n",
    "        child = os.path.join('%s%s' % (filepath, allDir))\n",
    "        files.append(child)\n",
    "    return files\n",
    "\n",
    "def get_text(path):\n",
    "    \"\"\"\n",
    "    get the text value in the file\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    text = open(path, 'r')\n",
    "    for line in text:\n",
    "        values.append(line)\n",
    "    return ' '.join(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = eachFile('movie/')\n",
    "movie_files = []\n",
    "for file in files:\n",
    "    if not file.find('.html') == -1:\n",
    "        movie_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# soup = bsoup(get_text(movie_files[0]))\n",
    "import nltk\n",
    "movie_text = []\n",
    "for file in movie_files:\n",
    "    htmlfile = open(file, 'r',encoding='ISO-8859-1')\n",
    "    try:\n",
    "        htmlpage = htmlfile.read()\n",
    "    except:\n",
    "        print(file)\n",
    "    soup = bsoup(htmlpage, \"html.parser\")\n",
    "    if (soup.find('p') is None):\n",
    "        movie_text.append('')\n",
    "    else:\n",
    "        sent = soup.find('p').string\n",
    "#         tokens=nltk.word_tokenize(str(sent))\n",
    "        movie_text.append(str(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "predict_data = pd.DataFrame(columns = ['file_name' , 'content'])\n",
    "predict_data['file_name'] = movie_files\n",
    "predict_data['content'] = movie_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "LancasterStem=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the training data\n",
    "files = eachFile('review_polarity/txt_sentoken/neg/')\n",
    "neg_files = []\n",
    "for file in files:\n",
    "    if not file.find('.txt') == -1:\n",
    "        neg_files.append(file)\n",
    "\n",
    "files = eachFile('review_polarity/txt_sentoken/pos/')\n",
    "pos_files = []\n",
    "for file in files:\n",
    "    if not file.find('.txt') == -1:\n",
    "        pos_files.append(file)\n",
    "\n",
    "neg_text = []\n",
    "pos_text = []\n",
    "for file in neg_files:\n",
    "    text = get_text(file)\n",
    "    neg_text.append(text)\n",
    "for file in pos_files:\n",
    "    text = get_text(file)\n",
    "    pos_text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the category of the word by using nltk method\n",
    "from nltk.corpus import stopwords\n",
    "tags = set(['NN', 'NNP', 'NNS', 'NNPS', 'VB', 'VBG', 'VBN', 'VBP', 'VBZ', 'RB', 'RBR', 'RBS', 'JJ', 'JJR', 'JJS'])\n",
    "def filter(text):\n",
    "    #using nltk to filter the stopword\n",
    "    sent = nltk.word_tokenize(text)\n",
    "    words = [LancasterStem.stem(w) for w in sent if w not in stopwords.words('english')]\n",
    "#     get the categoyr of word\n",
    "    pos_tags =nltk.pos_tag(words)\n",
    "    ret = []\n",
    "    #filter\n",
    "    for word,pos in pos_tags:\n",
    "        if (pos in tags):\n",
    "            ret.append(word)\n",
    "    return ' '.join(ret)\n",
    "#     return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the text after filtering\n",
    "neg_txt = list(map(filter, neg_text))\n",
    "pos_txt = list(map(filter, pos_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(neg_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# structer the data\n",
    "neg_pd = pd.DataFrame(columns = ['content', 'label'])\n",
    "pos_pd = pd.DataFrame(columns = ['content', 'label'])\n",
    "neg_pd['content'] = neg_txt\n",
    "neg_pd['label'] = np.ones((1000,1), dtype = np.int)\n",
    "pos_pd['content'] = pos_txt\n",
    "pos_pd['label'] = np.zeros((1000,1), dtype = np.int)\n",
    "data = pd.concat([pos_pd, neg_pd], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "count_vec = CountVectorizer()\n",
    "# cross validation\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.content, data.label, test_size=0.75, random_state=23)\n",
    "# word count by CountVectorizer\n",
    "x_train_mnb = count_vec.fit_transform(x_train)\n",
    "x_test_mnb = count_vec.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.94      0.75       740\n",
      "          1       0.89      0.45      0.60       760\n",
      "\n",
      "avg / total       0.76      0.69      0.68      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "# calculate the tfidf of the text \n",
    "tfidf = TfidfTransformer()\n",
    "x_train_tf = tfidf.fit_transform(x_train_mnb)\n",
    "x_test_tf = tfidf.transform(x_test_mnb)\n",
    "\n",
    "#predict by native bayes\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(x_train_tf, y_train)\n",
    "print(classification_report(y_test, mnb.predict(x_test_tf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.83      0.77       740\n",
      "          1       0.81      0.68      0.74       760\n",
      "\n",
      "avg / total       0.76      0.76      0.76      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm  \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#predict by SVM\n",
    "dtc = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)\n",
    "\n",
    "dtc.fit(x_train_mnb, y_train)\n",
    "print(classification_report(y_test, dtc.predict(x_test_mnb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.79      0.75       740\n",
      "          1       0.77      0.70      0.73       760\n",
      "\n",
      "avg / total       0.75      0.74      0.74      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbdt=GradientBoostingClassifier(n_estimators=200) \n",
    "\n",
    "#predict by GBDT\n",
    "gbdt.fit(x_train_tf, y_train)\n",
    "print(classification_report(y_test, gbdt.predict(x_test_tf.toarray())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
       "       penalty='l2', power_t=0.5, random_state=42, shuffle=True, verbose=0,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the perfomance of SVM is best, let's start predict\n",
    "count_vect = CountVectorizer()\n",
    "tf_idf = TfidfTransformer()\n",
    "X = data.content\n",
    "Y = data.label\n",
    "x_ct = count_vect.fit_transform(X)\n",
    "x_tf = tf_idf.fit_transform(x_ct)\n",
    "svm = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)\n",
    "svm.fit(x_tf, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_predict = predict_data['content']\n",
    "x_predict = x_predict.map(filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_predict_ct = count_vect.transform(x_predict)\n",
    "x_predict_tf = tf_idf.transform(x_predict_ct)\n",
    "predict = svm.predict(x_predict_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_data['sentiment'] = predict\n",
    "predict_data.loc[predict_data.sentiment == 0, 'sentiment'] = 'negative'\n",
    "predict_data.loc[predict_data.sentiment == 1, 'sentiment'] = 'positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>movie/0002.html</td>\n",
       "      <td>[Editor's note: Sites running 2.10 netnews wil...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>movie/0003.html</td>\n",
       "      <td>Set in the offices of two psychoanalysts,...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movie/0004.html</td>\n",
       "      <td>Starring: Mickey Rourke, Robert DeNiro, Lisa B...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>movie/0005.html</td>\n",
       "      <td>Harry Angel (played by Mickey Rourke) is ...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>movie/0006.html</td>\n",
       "      <td>I've been a little lax in getting out mov...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         file_name                                            content  label  \\\n",
       "0  movie/0002.html  [Editor's note: Sites running 2.10 netnews wil...      1   \n",
       "1  movie/0003.html       Set in the offices of two psychoanalysts,...      1   \n",
       "2  movie/0004.html  Starring: Mickey Rourke, Robert DeNiro, Lisa B...      0   \n",
       "3  movie/0005.html       Harry Angel (played by Mickey Rourke) is ...      1   \n",
       "4  movie/0006.html       I've been a little lax in getting out mov...      1   \n",
       "\n",
       "  sentiment  \n",
       "0  positive  \n",
       "1  positive  \n",
       "2  negative  \n",
       "3  positive  \n",
       "4  positive  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_data.to_csv('result.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
